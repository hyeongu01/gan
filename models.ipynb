{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1-2): 2 x Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=784, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, hidden_size=[512, 512, 512], output_size=784):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        self.hidden_layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_size[0]),\n",
    "                nn.BatchNorm1d(hidden_size[0]),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for i in range(len(hidden_size) - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size[i], hidden_size[i+1]),\n",
    "                    nn.BatchNorm1d(hidden_size[i+1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size[-1], output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "gen = Generator()\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - input data\n",
      "size: torch.Size([64, 100])\n",
      "최소값: -0.9995927810668945\n",
      "최대값: 0.999503493309021\n",
      " - output data\n",
      "size: torch.Size([64, 784])\n",
      "최소값: -0.9509106874465942\n",
      "최대값: 0.9578351974487305\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(64, 100) * 2 - 1  # [0, 1] → [-1, 1]\n",
    "\n",
    "# 확인\n",
    "print(\" - input data\")\n",
    "print(\"size:\", z.shape)\n",
    "print(\"최소값:\", z.min().item())\n",
    "print(\"최대값:\", z.max().item())\n",
    "\n",
    "output = gen(z)\n",
    "print(\" - output data\")\n",
    "print(\"size:\", output.shape)\n",
    "print(\"최소값:\", output.min().item())\n",
    "print(\"최대값:\", output.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxout(\n",
      "  (mo_layers): ModuleList(\n",
      "    (0-2): 3 x Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      " -- Maxout class test. --\n",
      "input: torch.Size([64, 512])\n",
      "output: torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# maxout class\n",
    "class Maxout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, size_units=2, num_mo_layers=3, dropout_prob=0.5):\n",
    "        super(Maxout, self).__init__()\n",
    "        \n",
    "        self.mo_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_mo_layers):\n",
    "            self.mo_layers.append(nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size*size_units),\n",
    "                nn.BatchNorm1d(hidden_size*size_units)\n",
    "            ))\n",
    "        self.size_units = size_units\n",
    "        self.num_mo_layers = num_mo_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ini_x = [layer(x) for layer in self.mo_layers]\n",
    "        for i in range(self.num_mo_layers):\n",
    "            ini = ini_x[i].view(ini_x[i].size(0), -1, self.size_units)\n",
    "            ini_x[i], _ = ini.max(dim=2)\n",
    "            ini_x[i] = self.dropout(ini_x[i])\n",
    "        output = torch.cat(ini_x, dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# maxout class test\n",
    "maxout = Maxout(512, 256)\n",
    "print(maxout)\n",
    "\n",
    "input_tensor = torch.rand(64, 512) # [64, 512]\n",
    "\n",
    "output = maxout(input_tensor)\n",
    "print(\" -- Maxout class test. --\")\n",
    "print(f\"input: {input_tensor.shape}\\noutput: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (input_layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Maxout(\n",
      "      (mo_layers): ModuleList(\n",
      "        (0-2): 3 x Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Maxout(\n",
      "      (mo_layers): ModuleList(\n",
      "        (0-2): 3 x Sequential(\n",
      "          (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      " -- Discriminator test -- \n",
      "input size: torch.Size([64, 784])\n",
      "output size: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Discriminator with Maxout class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=[512, 256, 128], num_mo_layer=3, output_size=1, dropout_prob=0.5):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size[0]),\n",
    "            nn.BatchNorm1d(hidden_size[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_size) - 1):\n",
    "            if i == 0:\n",
    "                in_size = hidden_size[i]\n",
    "            else:\n",
    "                in_size = hidden_size[i] * 3\n",
    "            self.hidden_layers.append(\n",
    "                Maxout(input_size=in_size, hidden_size=hidden_size[i+1], dropout_prob=dropout_prob)\n",
    "            )\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size[-1]*num_mo_layer, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Discriminator test    \n",
    "dis = Discriminator()\n",
    "print(dis)\n",
    "\n",
    "input_tensor = torch.rand(64, 784)\n",
    "output_tensor = dis(input_tensor)\n",
    "print(\" -- Discriminator test -- \")\n",
    "print(f\"input size: {input_tensor.shape}\\noutput size: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gan 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Gen_loss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(Gen_loss, self).__init__()\n",
    "        self.is_early = True\n",
    "        self.eps=eps\n",
    "        \n",
    "    def set_phase(self, is_early):\n",
    "        self.is_early = is_early\n",
    "        \n",
    "    def forward(self, d_gz):\n",
    "        if self.is_early:\n",
    "            loss = -torch.log(d_gz + self.eps).mean()\n",
    "        else:\n",
    "            loss = torch.log(1 - d_gz + self.eps).mean()\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "class Dis_loss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(Dis_loss, self).__init__()\n",
    "        self.eps=eps\n",
    "        \n",
    "        \n",
    "    def forward(self, d_x, d_gz):\n",
    "        x_loss = -torch.log(d_x + self.eps).mean()\n",
    "        z_loss = -torch.log(1 - d_gz + self.eps).mean()\n",
    "        loss = x_loss + z_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.gen_dis import build_discriminator, build_generator\n",
    "\n",
    "# from models.utils import Gen_loss, Dis_loss\n",
    "\n",
    "class Gan(nn.Module):\n",
    "    def __init__(self, k=1, device=\"cpu\"):\n",
    "        super(Gan, self).__init__()\n",
    "        \n",
    "        self.generator = build_generator().to(device)\n",
    "        self.discriminator = build_discriminator().to(device)\n",
    "        \n",
    "        self.k = k\n",
    "        self.device = device\n",
    "        \n",
    "        self.gen_loss = Gen_loss()\n",
    "        self.dis_loss = Dis_loss()\n",
    "        \n",
    "        self.optim_g = torch.optim.SGD(self.generator.parameters(), lr=0.0001, momentum=0.9)\n",
    "        self.optim_d = torch.optim.SGD(self.discriminator.parameters(), lr=0.0001, momentum=0.9)\n",
    "        \n",
    "    \n",
    "    def set_loss(self, gen_loss, dis_loss):\n",
    "        self.gen_loss = gen_loss\n",
    "        self.dis_loss = dis_loss\n",
    "    \n",
    "    def set_optimizer(self, optimizer_g, optimizer_d):\n",
    "        self.optim_g = optimizer_g\n",
    "        self.optim_d = optimizer_d\n",
    "        \n",
    "    def train_one_epoch(self, dataLoader, epoch=0):\n",
    "        # train discriminator k steps\n",
    "        self.discriminator.train()\n",
    "        self.generator.eval()\n",
    "        \n",
    "        epoch_loss_d = 0.0\n",
    "        for k in range(self.k):\n",
    "            with tqdm(dataLoader, unit=\"batch\", leave=False) as tepoch:\n",
    "                for x in tepoch:\n",
    "                    tepoch.set_description(f\"Epoch {epoch+1} | discriminator {k+1}/{self.k}\")\n",
    "\n",
    "                    self.optim_d.zero_grad()\n",
    "                    \n",
    "                    x = x.to(self.device)\n",
    "                    \n",
    "                    z = torch.randn(64, 100).to(self.device)\n",
    "                    gz = self.generator(z)\n",
    "                    d_gz = self.discriminator(gz)\n",
    "                    dx = self.discriminator(x)\n",
    "                    \n",
    "                    loss_d = self.dis_loss(dx, d_gz)\n",
    "                    loss_d.backward()\n",
    "                    \n",
    "                    self.optim_d.step()\n",
    "                    epoch_loss_d += loss_d.item()\n",
    "\n",
    "        epoch_loss_d = epoch_loss_d / self.k\n",
    "\n",
    "        \n",
    "        # train generator one steps\n",
    "        self.generator.train()\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "        epoch_loss_g = 0.0\n",
    "        with tqdm(dataLoader, unit=\"batch\", leave=False) as tepoch:\n",
    "            for x in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1} | generator\")\n",
    "\n",
    "                self.optim_g.zero_grad()\n",
    "                x = x.to(self.device)\n",
    "                \n",
    "                z = torch.randn(64, 100).to(self.device)\n",
    "                gz = self.generator(z)\n",
    "                d_gz = self.discriminator(gz)\n",
    "                \n",
    "                loss_g = self.gen_loss(d_gz)\n",
    "                loss_g.backward()\n",
    "                \n",
    "                self.optim_g.step()\n",
    "                epoch_loss_g += loss_g.item()\n",
    "        \n",
    "        return epoch_loss_d, epoch_loss_g\n",
    "    \n",
    "    def train(self, trainLoader, epochs, log_path=\"experiment_01\", early_rate=0.1):\n",
    "        writer = SummaryWriter(log_dir=f\"./runs/{log_path}\")\n",
    "        early_epoch = int(epochs * early_rate)\n",
    "        print(early_epoch)\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > early_epoch:\n",
    "                self.gen_loss.set_phase(False)\n",
    "                \n",
    "            train_loss_d, train_loss_g = self.train_one_epoch(trainLoader, epoch=epoch)\n",
    "            writer.add_scalar(\"Loss/Discriminator\", train_loss_d, epoch)\n",
    "            writer.add_scalar(\"Loss/Generator\", train_loss_g, epoch)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] train_loss_d: {train_loss_d}, train_loss_g: {train_loss_g}\")\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets import MNISTDataset, transform\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = MNISTDataset('./mnist', train=True, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100%7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] train_loss_d: 1323.9593309164047, train_loss_g: 195.78534737974405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gan \u001b[38;5;241m=\u001b[39m Gan(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 95\u001b[0m, in \u001b[0;36mGan.train\u001b[1;34m(self, trainLoader, epochs, log_path, early_rate)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m early_epoch:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_loss\u001b[38;5;241m.\u001b[39mset_phase(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m train_loss_d, train_loss_g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/Discriminator\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss_d, epoch)\n\u001b[0;32m     97\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/Generator\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss_g, epoch)\n",
      "Cell \u001b[1;32mIn[41], line 55\u001b[0m, in \u001b[0;36mGan.train_one_epoch\u001b[1;34m(self, dataLoader, epoch)\u001b[0m\n\u001b[0;32m     52\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(x)\n\u001b[0;32m     54\u001b[0m loss_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdis_loss(dx, d_gz)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mloss_d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_d\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m epoch_loss_d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_d\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\cdy52\\anaconda3\\envs\\tedy\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cdy52\\anaconda3\\envs\\tedy\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cdy52\\anaconda3\\envs\\tedy\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan = Gan(device=\"cuda:0\")\n",
    "gan.train(trainLoader=dataloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1321.750668644905, 191.3832784742117)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan = Gan(device=\"cuda:0\")\n",
    "gan.train_one_epoch(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218.3548020124435, 113.96270113438368)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.train_one_epoch(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1051.3563024401665, 38.42546373791993)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m, in \u001b[0;36mGan.train_one_epoch\u001b[0;34m(self, dataLoader)\u001b[0m\n\u001b[1;32m     70\u001b[0m d_gz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(gz)\n\u001b[1;32m     72\u001b[0m loss_g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_loss(d_gz)\n\u001b[0;32m---> 73\u001b[0m \u001b[43mloss_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_g\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m epoch_loss_g \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_g\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tedy/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tedy/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(gan.train_one_epoch(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(64, 100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tedy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
