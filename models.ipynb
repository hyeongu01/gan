{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=784, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tedy/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, hidden_size=[512, 512, 512], output_size=784):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        self.hidden_layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_size[0]),\n",
    "                nn.BatchNorm1d(hidden_size[0]),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for i in range(len(hidden_size) - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size[i], hidden_size[i+1]),\n",
    "                    nn.BatchNorm1d(hidden_size[i+1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size[-1], output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "gen = Generator()\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - input data\n",
      "size: torch.Size([64, 100])\n",
      "최소값: -0.9996590614318848\n",
      "최대값: 0.999913215637207\n",
      " - output data\n",
      "size: torch.Size([64, 784])\n",
      "최소값: -0.9509567618370056\n",
      "최대값: 0.9532948136329651\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(64, 100) * 2 - 1  # [0, 1] → [-1, 1]\n",
    "\n",
    "# 확인\n",
    "print(\" - input data\")\n",
    "print(\"size:\", z.shape)\n",
    "print(\"최소값:\", z.min().item())\n",
    "print(\"최대값:\", z.max().item())\n",
    "\n",
    "output = gen(z)\n",
    "print(\" - output data\")\n",
    "print(\"size:\", output.shape)\n",
    "print(\"최소값:\", output.min().item())\n",
    "print(\"최대값:\", output.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxout(\n",
      "  (mo_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      " -- Maxout class test. --\n",
      "input: torch.Size([64, 512])\n",
      "output: torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# maxout class\n",
    "class Maxout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, size_units=2, num_mo_layers=3, dropout_prob=0.5):\n",
    "        super(Maxout, self).__init__()\n",
    "        \n",
    "        self.mo_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_mo_layers):\n",
    "            self.mo_layers.append(nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size*size_units),\n",
    "                nn.BatchNorm1d(hidden_size*size_units)\n",
    "            ))\n",
    "        self.size_units = size_units\n",
    "        self.num_mo_layers = num_mo_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ini_x = [layer(x) for layer in self.mo_layers]\n",
    "        for i in range(self.num_mo_layers):\n",
    "            ini = ini_x[i].view(ini_x[i].size(0), -1, self.size_units)\n",
    "            ini_x[i], _ = ini.max(dim=2)\n",
    "            ini_x[i] = self.dropout(ini_x[i])\n",
    "        output = torch.cat(ini_x, dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# maxout class test\n",
    "maxout = Maxout(512, 256)\n",
    "print(maxout)\n",
    "\n",
    "input_tensor = torch.rand(64, 512) # [64, 512]\n",
    "\n",
    "output = maxout(input_tensor)\n",
    "print(\" -- Maxout class test. --\")\n",
    "print(f\"input: {input_tensor.shape}\\noutput: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (input_layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Maxout(\n",
      "      (mo_layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Maxout(\n",
      "      (mo_layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      " -- Discriminator test -- \n",
      "input size: torch.Size([64, 784])\n",
      "output size: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Discriminator with Maxout class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=[512, 256, 128], num_mo_layer=3, output_size=1, dropout_prob=0.5):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size[0]),\n",
    "            nn.BatchNorm1d(hidden_size[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_size) - 1):\n",
    "            if i == 0:\n",
    "                in_size = hidden_size[i]\n",
    "            else:\n",
    "                in_size = hidden_size[i] * 3\n",
    "            self.hidden_layers.append(\n",
    "                Maxout(input_size=in_size, hidden_size=hidden_size[i+1], dropout_prob=dropout_prob)\n",
    "            )\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size[-1]*num_mo_layer, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Discriminator test    \n",
    "dis = Discriminator()\n",
    "print(dis)\n",
    "\n",
    "input_tensor = torch.rand(64, 784)\n",
    "output_tensor = dis(input_tensor)\n",
    "print(\" -- Discriminator test -- \")\n",
    "print(f\"input size: {input_tensor.shape}\\noutput size: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gan 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Gen_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gen_loss, self).__init__()\n",
    "        self.is_early = True\n",
    "        \n",
    "    def set_phase(self, is_early):\n",
    "        self.is_early = is_early\n",
    "        \n",
    "    def forward(self, d_gz):\n",
    "        if self.is_early:\n",
    "            loss = -torch.log(d_gz).mean()\n",
    "        else:\n",
    "            loss = torch.log(1 - d_gz).mean()\n",
    "        return loss\n",
    "    \n",
    "class Dis_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dis_loss, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, d_x, d_gz):\n",
    "        x_loss = -torch.log(d_x).mean()\n",
    "        z_loss = -torch.log(1 - d_gz).mean()\n",
    "        loss = x_loss + z_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.gen_dis import build_discriminator, build_generator\n",
    "\n",
    "# from models.utils import Gen_loss, Dis_loss\n",
    "\n",
    "class Gan(nn.Module):\n",
    "    def __init__(self, k=1, device=\"cpu\"):\n",
    "        super(Gan, self).__init__()\n",
    "        \n",
    "        self.generator = build_generator().to(device)\n",
    "        self.discriminator = build_discriminator().to(device)\n",
    "        \n",
    "        self.k = k\n",
    "        self.device = device\n",
    "        \n",
    "        self.gen_loss = Gen_loss()\n",
    "        self.dis_loss = Dis_loss()\n",
    "        \n",
    "        self.optim_g = torch.optim.SGD(self.generator.parameters(), lr=0.0001, momentum=0.9)\n",
    "        self.optim_d = torch.optim.SGD(self.discriminator.parameters(), lr=0.0001, momentum=0.9)\n",
    "        \n",
    "    \n",
    "    def set_loss(self, gen_loss, dis_loss):\n",
    "        self.gen_loss = gen_loss\n",
    "        self.dis_loss = dis_loss\n",
    "    \n",
    "    def set_optimizer(self, optimizer_g, optimizer_d):\n",
    "        self.optim_g = optimizer_g\n",
    "        self.optim_d = optimizer_d\n",
    "        \n",
    "    def train_one_epoch(self, dataLoader):\n",
    "        # train discriminator k steps\n",
    "        self.discriminator.train()\n",
    "        self.generator.eval()\n",
    "        \n",
    "        epoch_loss_d = 0.0\n",
    "        for k in range(self.k):\n",
    "            for x in dataLoader:\n",
    "                self.optim_d.zero_grad()\n",
    "                \n",
    "                x = x.to(self.device)\n",
    "                \n",
    "                z = torch.randn(64, 100).to(self.device)\n",
    "                gz = self.generator(z)\n",
    "                d_gz = self.discriminator(gz)\n",
    "                dx = self.discriminator(x)\n",
    "                \n",
    "                loss_d = self.dis_loss(dx, d_gz)\n",
    "                loss_d.backward()\n",
    "                \n",
    "                self.optim_d.step()\n",
    "                epoch_loss_d += loss_d.item()\n",
    "            epoch_loss_d = epoch_loss_d / self.k\n",
    "        \n",
    "        # train generator one steps\n",
    "        self.generator.train()\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "        epoch_loss_g = 0.0\n",
    "        for x in dataLoader:\n",
    "            self.optim_g.zero_grad()\n",
    "            x = x.to(self.device)\n",
    "            \n",
    "            z = torch.randn(64, 100).to(self.device)\n",
    "            gz = self.generator(z)\n",
    "            d_gz = self.discriminator(gz)\n",
    "            \n",
    "            loss_g = self.gen_loss(d_gz)\n",
    "            loss_g.backward()\n",
    "            \n",
    "            self.optim_g.step()\n",
    "            epoch_loss_g += loss_g.item()\n",
    "        \n",
    "        return epoch_loss_d, epoch_loss_g\n",
    "    \n",
    "    def train(self, trainLoader, epochs, early_rate=0.1):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets import MNISTDataset, transform\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = MNISTDataset('./mnist', train=True, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1317.151783466339, 164.3575006350875)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan = Gan()\n",
    "gan.train_one_epoch(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218.3548020124435, 113.96270113438368)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.train_one_epoch(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1051.3563024401665, 38.42546373791993)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m, in \u001b[0;36mGan.train_one_epoch\u001b[0;34m(self, dataLoader)\u001b[0m\n\u001b[1;32m     70\u001b[0m d_gz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(gz)\n\u001b[1;32m     72\u001b[0m loss_g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_loss(d_gz)\n\u001b[0;32m---> 73\u001b[0m \u001b[43mloss_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_g\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m epoch_loss_g \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_g\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tedy/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tedy/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(gan.train_one_epoch(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(64, 100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tedy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
